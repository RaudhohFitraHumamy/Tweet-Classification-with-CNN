{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "CNN2_tweet.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFzSlWeKiKZ5",
        "outputId": "151dfba1-7a20-4704-df7d-30b8b574f86c"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from pickle import dump\n",
        "nltk.download('stopwords')\n",
        "from string import punctuation"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvECqd7siKaE"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "stop_words = set(stopwords.words('indonesian'))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yeQDS0K_m5x"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLaKPkNC_oLp",
        "outputId": "c89b695c-7f23-46a9-f434-bd1b17458bb7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn1905fHiKaF",
        "outputId": "f131fd0d-202d-4390-a098-9aee84ddb964"
      },
      "source": [
        "dataset = [j for j in open('/content/drive/MyDrive/Bahan Tugas Akhir/CNN/dataset/tagged_data.txt',encoding=\"utf8\").read().split(\"\\n\") if j !='']\n",
        "print(dataset[:11])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rt @napqilla: no 1, 3 ambisinya menguasai rakyat, no.2 ambisinya melayani rakyat, itu terlihat #debat2pilkadadki, mknya #loguepilihbadja aja;1', 'rt @pandji: nah gue pikir sentimen petahana oke di #debat2pilkadadki semalam. tapi mnrt politicawave malah sebaliknya. geng\\\\u2026 ;1', 'rt @pandji: urutan pertama best moment #debat2pilkadadki : pak basuki misahin bu sylvie &amp;1', 'rt @pandji: ini artikel yg menjelaskan ternyata di #debat2pilkadadki yg dapet respon positif di socmed adalah #aniessandiokoce https:\\\\/\\\\/t.co\\\\u2026;1', 'rt @mrtampi: agus makin santai.\\\\nahok makin santun.\\\\nanies makin esmosi.\\\\n\\\\nimho.\\\\n#debat2pilkadadki;0', 'pilkada dki jangan pilih pki!! berbahaya!! pilih 1 atau 3 saja tahun baru imlek, pkb aj... https:\\\\/\\\\/t.co\\\\/ggych4sgct #debat2pilkadadki #imlek;0', 'pdip sengaja becah belah rakyat warga dki tidak pilih ahok!! wni ang... ;0', 'nama nya juga debat\\\\nbiasa nya itu https:\\\\/\\\\/t.co\\\\/cjertypdxp;1', 'rt @dadansuwarna: kita ttp bisa bijak memilih, tanpa harus berteriak, tanpa harus mengumpat. #debat2pilkadadki #debatcagubdki;1', 'rt @aniesbaswedan: ibu, terima kasih atas doa dan kumpulan pelajaran yang telah membangun pemikiran, karakter dan semangat untuk berju\\\\u2026 ;1', 'rt @gunromli: sylviana kesyikan ngomong sendiri lupa bertanya, anies keasyikan nyerang pake data2 yg salah, ya ahok joged2 aja \\\\ud83d\\\\ude01 #debat2pil\\\\u2026;0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kpNFt30iKaF"
      },
      "source": [
        "#preprocess per sentence\n",
        "def clean_doc(doc):\n",
        "    #split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    #remove punctuation from each token\n",
        "    table  = str.maketrans('','',punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    #remove remaining tokens that not alphabetic\n",
        "    #tokens = [word for word in tokens if word.isalpha()]\n",
        "    #filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    #filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6ljxOBdiKaH",
        "outputId": "14db01fc-6869-4d26-a9a3-1777c14834de"
      },
      "source": [
        "dataset_cln = []\n",
        "positive_dt =[]\n",
        "negative_dt =[]\n",
        "\n",
        "for line in dataset:\n",
        "    line = line.split(\";\")\n",
        "    if line[1] == \"1\":\n",
        "        positive_dt.append(clean_doc(line[0]))\n",
        "    else:\n",
        "        negative_dt.append(clean_doc(line[0]))\n",
        "    \n",
        "print(len(positive_dt))\n",
        "print(len(negative_dt))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "757\n",
            "749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11OLRg3TiKaH",
        "outputId": "66b497b6-6c9c-4d58-cc97-0fa56b551b80"
      },
      "source": [
        "positive_dt[:11]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt napqilla ambisinya menguasai rakyat no2 ambisinya melayani rakyat itu terlihat debat2pilkadadki mknya loguepilihbadja aja',\n",
              " 'rt pandji nah gue pikir sentimen petahana oke di debat2pilkadadki semalam tapi mnrt politicawave malah sebaliknya gengu2026',\n",
              " 'rt pandji urutan pertama best moment debat2pilkadadki pak basuki misahin bu sylvie amp',\n",
              " 'rt pandji ini artikel yg menjelaskan ternyata di debat2pilkadadki yg dapet respon positif di socmed adalah aniessandiokoce httpstcou2026',\n",
              " 'nama nya juga debatnbiasa nya itu httpstcocjertypdxp',\n",
              " 'rt dadansuwarna kita ttp bisa bijak memilih tanpa harus berteriak tanpa harus mengumpat debat2pilkadadki debatcagubdki',\n",
              " 'rt aniesbaswedan ibu terima kasih atas doa dan kumpulan pelajaran yang telah membangun pemikiran karakter dan semangat untuk berjuu2026',\n",
              " 'rt jokoanwar tiap ahok djarot ngomong nambah ilmu ya kita debat2pilkadadki httpstcoxwwrnalpbu',\n",
              " 'rt jessyastrid makasih yosimokalu jangan kasih kendor untuk 1putaran 2periode debat2pilkadadki salam dua jadiiiiu270cu270c httpstcojb5lau2026',\n",
              " 'rt kimochiii ud83cudfa4mohon bersabar ini ujian ud83dude14debat2pilkadadki httpstcoihw6yznay4',\n",
              " 'rt teddygusnaidi 33 inilah gambaran yg saya sampaikan sebagai pendidikan politik agar supaya masyarakat tau siapa agus amp']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo5iuaGhiKaI"
      },
      "source": [
        "trainX = negative_dt[:500] + positive_dt[:500]\n",
        "#labeling dataset, for first length of negative post, label with 0 for next length of positive post, label with 1\n",
        "trainy = [0 for _ in range(500)] + [1 for _ in range(500)]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iqwTwIViKaI"
      },
      "source": [
        "# trainX[:11]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqPeo6K_iKaJ",
        "outputId": "3c20915e-4b7a-4f49-827c-40bc165353a9"
      },
      "source": [
        "def save_dataset(dataset, filename):\n",
        "    dump(dataset, open(filename,'wb'))\n",
        "    print('saved %s'% filename)\n",
        "\n",
        "save_dataset([trainX,trainy], 'train.pkl')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved train.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WNiu5OriKaK",
        "outputId": "978f6ca0-d897-4b44-90a9-cbb301c71bd7"
      },
      "source": [
        "testX = negative_dt[501:] + positive_dt[501:]\n",
        "testY = [0 for _ in range(len(negative_dt[501:]))] + [1 for _ in range(len(positive_dt[501:]))]\n",
        "save_dataset([testX,testY],'test.pkl')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjIPZNNe7VhO"
      },
      "source": [
        "# testX"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqMKA8VpiKaK"
      },
      "source": [
        "import pickle\n",
        "\n",
        "def load_dataset(filename):\n",
        "    return pickle.load(open(filename,'rb'))\n",
        "\n",
        "trainLines, trainLabels = load_dataset('train.pkl')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05qAah6-iKaL",
        "outputId": "11c12bf4-acc6-4003-eb39-62803249ca7a"
      },
      "source": [
        "len(trainLines)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdhHBVQNiKaL"
      },
      "source": [
        "# trainLabels"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgkmFxegiKaL"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpgT7wxIiKaM"
      },
      "source": [
        "#load the cleaned dataset\n",
        "def load_dataset(filename):\n",
        "    return pickle.load(open(filename,'rb'))\n",
        "\n",
        "#fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def max_length(lines):\n",
        "    return max([len(s.split()) for s in lines])\n",
        "\n",
        "def encode_text(tokenizer, lines, length):\n",
        "    #integer encoded\n",
        "    encoded = tokenizer.texts_to_sequences(lines)\n",
        "    #pad encoded sequences\n",
        "    padded = pad_sequences(encoded, maxlen = length, padding='post')\n",
        "    return padded\n",
        "\n",
        "def define_model(length, vocab_size):\n",
        "    #channel 1\n",
        "    inputs1 = Input(shape=(length,))\n",
        "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "    drop1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "    #channel 2\n",
        "    inputs2 = Input(shape=(length,))\n",
        "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "    conv2 = Conv1D(filters=32, kernel_size = 6, activation='relu')(embedding2)\n",
        "    drop2 = Dropout(0.5)(conv2)\n",
        "    pool2 = Dropout(0.5)(conv2)\n",
        "    flat2 = Flatten()(pool2)\n",
        "    \n",
        "    #channel 3\n",
        "    inputs3 = Input(shape=(length,))\n",
        "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "    drop3 = Dropout(0.5)(conv3)\n",
        "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "    flat3 = Flatten()(pool3)\n",
        "    #merge\n",
        "    merged = concatenate([flat1, flat2, flat3])\n",
        "    \n",
        "    #interpretation\n",
        "    dense1 = Dense(10, activation='relu')(merged)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "    \n",
        "    #compile\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    #summarize\n",
        "    print(model.summary())\n",
        "    plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kfT35YsiKaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcaa0c85-d52d-4032-9a14-2c64754ee356"
      },
      "source": [
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "#create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "#calculate max document length\n",
        "length = max_length(trainLines)\n",
        "#calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "print('Max document length : %d' % length)\n",
        "print('vocabulary size : %d' % vocab_size)\n",
        "\n",
        "#encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "print(trainX.shape)\n",
        "\n",
        "#define model\n",
        "model = define_model(length, vocab_size)\n",
        "#fit model\n",
        "model.fit([trainX, trainX, trainX], array(trainLabels), epochs=10, batch_size = 16)\n",
        "#save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length : 39\n",
            "vocabulary size : 3427\n",
            "(1000, 39)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 39)]         0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 39)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 39, 100)      342700      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 39)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 39, 100)      342700      ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 36, 32)       12832       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 39, 100)      342700      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 32, 32)       25632       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 36, 32)       0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 34, 32)       19232       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 32, 32)       0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 18, 32)       0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 34, 32)       0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 16, 32)      0           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 576)          0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 1088)         0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 512)          0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2176)         0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           21770       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            11          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,107,577\n",
            "Trainable params: 1,107,577\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - 3s 23ms/step - loss: 0.6479 - accuracy: 0.6570\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 0.3823 - accuracy: 0.8790\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 1s 23ms/step - loss: 0.1963 - accuracy: 0.9390\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 0.1679 - accuracy: 0.9350\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 0.1408 - accuracy: 0.9530\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 0.1220 - accuracy: 0.9440\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 1s 23ms/step - loss: 0.1107 - accuracy: 0.9560\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 0.1095 - accuracy: 0.9540\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 1s 22ms/step - loss: 0.1088 - accuracy: 0.9530\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 1s 23ms/step - loss: 0.1023 - accuracy: 0.9580\n"
          ]
        }
      ]
    }
  ]
}